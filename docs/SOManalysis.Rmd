---
title: <font size = "5"> Self-organizing map analysis of spring runoff from Mt Mansfield </font>
author: "Dustin Kincaid"
date: "1/12/2021<br><br>"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r notes, include=FALSE}
# A helpful website about Rmarkdown
  # https://ourcodingclub.github.io/tutorials/rmarkdown/
```

#### Objective  
Cluster daily environmental drivers to assess how they converge to drive spring runoff patterns from east-side Mt Mansfield watersheds
<br><br>

```{r setup, include=FALSE, warning=FALSE, message = FALSE}
# Set defaults
knitr::opts_chunk$set(fig.width = 6, fig.asp = 0.618, out.width = "70%", fig.align = "center") 

# Load packages ----
  library("here")      # to make file paths fail less
  library("tidyverse") # general workhorse
  library("lubridate") # to work with dates
  library("kohonen")   # to run the SOM
  library("vegan")     # to run adonis() for calc of nonparametric F test
  # devtools::install_github("laresbernardo/lares")
  library("lares")     # examine var correlations
  # may need to install 'lares' via: devtools::install_github("laresbernardo/lares")
  library("ggforce")   # for facet plots on multiple pages
  library("gt")       # print pretty tables
  library("cowplot")  # plotting assistance
  library("grid")     
  library("gridExtra")
  library("factoextra")  # for PCA plots
  library("zoo")         # for rolling functions

  # Load the range normalization function written by Kristin Underwood
  source(here("scripts", "L2norm.R"))

# Create a new folder to save the plots and CSVs produced by the SOM code
  newFolder <- "data/somResults/"
  dir.create(here(newFolder), recursive = TRUE)

# Give a site label to file names produced in SOM code
  myDataSet <- c("bothSites")

# Read in data
  # Calculated event metrics for each site as calculated in scripts/compileData.R
  mets <- read_csv(here("data", "alldata_compiled.csv"))
```

#### Shorten window of analysis to focus on spring melt period
```{r analysis window, include = TRUE, echo = TRUE, warning = FALSE, message = FALSE}
# First we're going to filter the data to focus on March-May dynamics
  # We want to avoid incorporating too much of the climate pattern, which would wash out this spring signal
  # We'll want to play with the size of this window
  # For reference water year days-of-year: Mar 15 = 166; Apr 1 = 183; May 1 = 213; May 15 = 227; May 31 = 243
  mets_sub <- mets %>%
    # filter(doy_wyear >= 160 & doy_wyear <= 250)
    filter(doy_wyear >= 170 & doy_wyear <= 245)
```
<br><br>

We eventually want to look at differences between cumulative runoff yields at the end of the water year so let's see if cumulative runoff yields at the end of the analysis window correlate with those at the end of the water year
<br><br>
```{r spr vs fall corr, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
# We eventually want to look at differences between cumulative runoff yields at the end of the water yield
# so let's see if cumulative runoff yields at the end of the analysis window correlate with those at the end of the water year
  yields_spr <- 
    mets_sub %>% 
    group_by(site, wyear) %>% 
    do(tail(., 1)) %>% 
    select(site, wyear, q_mm_cum) %>% 
    pivot_wider(names_from = site, values_from = q_mm_cum) %>% 
    mutate(WBminusRB_abs_spr = WB - RB,
           WBminusRB_per_spr = (WB - RB)/RB) %>% 
    select(-c(WB, RB))

  yields_end <- 
    mets %>% 
    group_by(site, wyear) %>% 
    do(tail(., 1)) %>% 
    filter(!is.na(site)) %>% 
    select(site, wyear, q_mm_cum) %>% 
    pivot_wider(names_from = site, values_from = q_mm_cum) %>% 
    mutate(WBminusRB_abs_end = WB - RB,
           WBminusRB_per_end = (WB - RB)/RB) %>% 
    select(-c(WB, RB))
  
  yields_comp <- 
    full_join(yields_spr, yields_end)
  
  yields_comp %>% 
    ggplot(aes(x = WBminusRB_abs_spr, y = WBminusRB_abs_end)) +
    geom_smooth(method = "lm") +
    geom_point() +
    geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
    xlab("Absolute diff. in spring") +
    ylab("Absolute diff. in fall") +
    theme_classic() +
    ggtitle("Comparing absolute yield differences in spring vs. \n end of water year for each watershed")
  
  r_abs <- round(cor.test(yields_comp$WBminusRB_abs_spr, yields_comp$WBminusRB_abs_end)$estimate, 2)
  p_abs <- round(cor.test(yields_comp$WBminusRB_abs_spr, yields_comp$WBminusRB_abs_end)$p.value, 3)
  noquote(paste0("r = ", r_abs))
  noquote(paste0("p = ", p_abs))
  
  yields_comp %>% 
    ggplot(aes(x = WBminusRB_per_spr, y = WBminusRB_per_end)) +
    geom_smooth(method = "lm") +
    geom_point() +
    geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
    xlab("Percent diff. in spring") +
    ylab("Percent diff. in fall") +
    theme_classic() +
    ggtitle("Comparing percent yield differences in spring vs. \n end of water year for each watershed")
  
  r_per <- round(cor.test(yields_comp$WBminusRB_per_spr, yields_comp$WBminusRB_per_end)$estimate, 2)
  p_per <- round(cor.test(yields_comp$WBminusRB_per_spr, yields_comp$WBminusRB_per_end)$p.value, 3)
  noquote(paste0("r = ", r_per))
  noquote(paste0("p = ", p_per))
```
<br><br>

#### Select variables to keep
If two variables are strongly correlated (negatively or positively) they can effectively "double-weight" a particular factor important in driving
clustering; thus, keep just one of the variables to serve as a proxy for that factor
```{r select vars, include = TRUE, echo = TRUE, warning = FALSE, message = FALSE, fig.width = 8, out.width = "80%"}
# For the analysis we only need one watershed/site of data, because the drivers we use are the same for both sites
  mets_sub_1site <- mets_sub %>% 
    filter(site == "WB")

# Select vars to keep Step 1
  # List the ID and response variables that will NOT be used in the analysis
  drop.info <- names(mets_sub_1site %>% 
                       select(# Remove the non-numerical ID/INFO
                              site, wyear, date, doy_wyear,
                              # Remove response variables
                              q_mm, q_mm_cum, q_mm_per,
                              # Remove other variables we know we won't want for SOM, but will want for graphing results later
                              precip_mm_cum, precip_mm_per))
  
  # List the other variables that will not be used in this analysis
  drop.vars <- names(mets_sub_1site %>% 
                       select(temp_max, temp_min,
                              # We'll keep the average temps measured at VWC/FEMC station, so drop those from Morrisville/Stowe airport
                              temp_avg,
                              # Let's just use the cumulative total volume of water diverted from the stream by the resort
                              use_snow_mm_daily, use_total_mm_daily,
                              # After looking at an analysis, I'm going to drop the use categories altogether,
                              use_total_mm_cum))  

# Look at correlations between variables
  mets_sub_1site %>% 
    # Remove the vars listed above
    select(-one_of(drop.info, drop.vars)) %>% 
    # Keep only complete observations/rows (no NAs in any of the columns)
    na.omit() %>% 
    # Visualize these
    lares::corr_cross(max_pvalue = 0.05, top = 20)
```
Not too worried about correlations
<br><br>

#### Self-organizing map (SOM)
##### Prepare data & set up grid/lattice dimensions  
We're only using complete observations/rows (no NAs in any columns)  
According to the heuristic rule from Vesanto 2000, number of grid elements/grid size/nodes = 5 * sqrt(n)  
To determine the the shape of the grid (ratio of columns to rows), we use the ratio of the first two eigen values of the input data set as recommended by Park et al. 2006  
```{r SOM setup, include = TRUE, echo = FALSE}
myData <- mets_sub_1site %>% 
  # Remove the vars listed above
  select(-one_of(drop.info, drop.vars)) %>% 
  # Keep only complete observations/rows (no NAs in any of the columns)
  na.omit()

noquote(paste0("No. of complete observations: ", nobs(myData), " out of ", nobs(mets_sub_1site), " observations"))

# Run this code once to determine which dimensions to input into the params_SOM . . .csv file that is saved as the 'params' object below
  # According to the heuristic rule from Vesanto 2000, number of grid elements/grid size/nodes = 5 * sqrt(n)
  VesantoNodes = round(5 * sqrt(nrow(myData)), 0)
  noquote(paste0("No. of Vesanto nodes: ", VesantoNodes))
  
  # To determine the the shape of the grid, we'll determine the ratio of columns to rows using the ratio of 
  # the first two eigen values of the input data set as recommended by Park et al. 2006
  
  # scale = TRUE = correlation matrix; scale = FALSE = covariance matrix
  PCA <- prcomp(myData, scale = TRUE, center = TRUE)
  eigenValues <- PCA$sdev^2
  first <- nth(eigenValues, 1)
  second <- nth(eigenValues, 2)
  gridRatio <- round((first/second), 1)
  noquote(paste0("Ratio of columns to rows: ", gridRatio))

  # Input this information into the params_SOM . . .csv file
```
<br><br>

##### Run SOM for a suite of grid/lattice configurations, # of nodes, and # of clusters
Code courtesy of Kristen Underwood (hidden)
```{r SOM, include = TRUE, echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE}
# SOM Code from Kristen Underwood's sedRegData_V2.R script
# Also adapted by referencing Brittney L's Run_SOM.R script
# ------------- Define Color Palettes of SOM plots  -------------------------------------------

# Colour palette definition - this added from blog comments (found in 2014-01 CSO_SOM.R script)
# Used later in clustering plot
pretty_palette <- c("#1f77b4", '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2')

# Palette defined by kohonen package, Shane Lynn 14-01-2014, code in zip file downloaded from link in above blog
# Used later in Component Plane (HeatMap) plots
coolBlueHotRed <- function(n, alpha = 1) {
      rainbow(n, end=4/6, alpha=alpha)[n:1]
}

#barPalette=c("gray", "red", "green3", "blue","cyan", "magenta", "yellow", "black")
barPalette=c("red", "orange1", "green3", "blue","cyan", "magenta", "yellow", "black")
sedRegPallette=c("cyan", "yellow", "orange", "red", "green3", "orchid","gray", "blue")    
    
# ------------- Set Grid parameters for various Runs  -------------------------------------------

# Read in table of parameters (grid rows, cols, grid type, learning rate etc)
### IMPORTANT: cannot have number nodes greater than num observations
params <- read_csv(here("Data", "somParams", "params_SOM_days170to245.csv"))
params <- as.matrix(params[1:nrow(params),])

# Vector of observation number from original dataset
observ <- myData %>% 
  mutate(obs = row_number()) %>% 
  dplyr::pull(obs)

# initialize matrix to store results of each SOM run
temp1 <- matrix(0, nrow(params), 5) #initialize empty matrix to append to params
Results <- cbind(params, temp1) # concatenate empty columns to copy of params matrix

# initialize matrix to store pattern to node assignments by run
clustAssignments <- matrix(0, 1, nrow(myData))

# initialize matrix to store pattern to node assignments by run
#clustMeans_Res <- matrix(0, 1, ncol(myData_num))

#loop through choices of params by Run number
# i <- 1  #use this to break for loop  #end for loop near line 410
for (i in 1:nrow(params)) { # original for loop starter
      #### Define SOM input parameters
      myRun <- params[i,1]
      myDataSet <- params[i,2]
      myTopo <- params[i,4]
      rows <- as.numeric(params[i,5])
      cols <- as.numeric(params[i,6])
      normMeth <- params[i,7]
      wghtMeth <- params[i,8]
      niter <- as.numeric(params[i,9])  #number of iterations, 100 is default
      crsAlpha <- as.numeric(params[i,10]) #learning rate: coarse tuning phase, 0.05 is default
      finAlpha <- as.numeric(params[i,11]) #learning rate: fine tuning phase, 0.01 is default
      nclusters <- as.numeric(params[i,12])
      #for now, rest of som parameters are default values
      
     
      # -----------------Normalize the data ---------------------------------
      ### output from either is a matrix, which is format required by som()
      
      if (normMeth == "scale") {
            # Method 1 - z transformation (subtract mean, divide by stdev)
            myData.sc <- scale(myData)  #scale is a base R function
            
      } else {
            # Method 2 - L2 normalization by variable (scale between 0 and 1)
            # normMeth <- c("L2")
            # I load this up top (note from DK)
            # source("L2norm.R")  #function has been created by KU
            myData.sc <- L2norm(myData)
      }
      
      # ----------------- Weight the inputs (Optional) ---------------------------------
      
      if (wghtMeth == "PCA") {
        ### Weight the data based on the PC loadings.
        # copy the data into a new object for weighting
        myData_wghtd <- myData.sc 
        
        # Multiply the individual variables by the weights:
        
        # this "for" loop goes through each column in the data 
        # which is going to go into the SOM, matches the column name to the
        # names of the PCA loadings for PC1, and multiplies those loadings 
        # by the scaled values in the column to get weighted values:
        
        #ADD a zero weight for variable L ??
        for (w in 1:ncol(myData_wghtd)) {
          myData_wghtd[,w]<-myData_wghtd[,w] * weightsPCA[which(names(weightsPCA) == colnames(myData_wghtd)[w])]
        }
        myData.sc <- myData_wghtd
      }
      
      # ---------------------- Run the SOM ----------------------------------
      
      #set.seed(12)  #to replicate results 
      myGrid <- somgrid(cols, rows, topo = myTopo)  #function somgrid is sourced from the "class" package
      som_model <- som(X = myData.sc, grid = myGrid, rlen = niter,
                       alpha=c(0.05,0.01), 
                       keep.data = TRUE)  #run the som
      
      #, mode = c("online")  #add this as arg to som()
      #, neighbourhood.fct = c("bubble")  #add this as arg to somgrid? no need, it is default
      
      # Calculate errors
      QE <- mean(som_model$distances)  # Quantization Error
      # kohonen_2017 now generates this automatically in summary() and print()
      
      # OLD kohonen_2015 way of calculating topographic error
      #TEdist <- topo.error(som_model, type = "nodedist") #another option
      #TEbmu <- topo.error(som_model, type = "bmu") # Topographic Error
      
      # NEW kohonen_2017 way  #### 4/10: this section still needs work
      obj.dists <- object.distances(som_model, type = "data")
      # Or create An object of class "dist", which can be directly fed into 
      # (e.g.) a hierarchical clustering.
      code.dists <- object.distances(som_model, type = "codes")
      
      un.dists <- unit.distances(myGrid)
      
      # check1 <- 'check1'
      # print(check1)
      
      # ------------------- Assign Nodes to a Cluster -------------------------------------
      
      #according to dissimilarity between trained weight vectors
      #given nClusters specified in params;
      #can review performance of SOM under various cluster # by doing mult runs
      #OLD CODE
      #som_cluster <- cutree(hclust(dist(som_model$codes)), nclusters)
      #NEW CODE
      som_cluster <- cutree(hclust(code.dists), nclusters)

      # check2 <- 'check2'
      # print(check2)    
      # -------------- Visualize results in various plots (exported to pdf) --------------------------
      # Create a new folder to save the SOM plots in PDFs produced by the SOM code
      newFolder2 <- paste("Data/somResults/", Sys.Date(), "/SOMplots_PDFs", sep = "")
      dir.create(here(newFolder2), recursive = TRUE)
      myFileName2 <- paste(myRun,"SOMplots",myDataSet,wghtMeth,normMeth, paste0(rows,"x", cols, myTopo), paste0(nclusters,"cl"), sep = "_")
      pdf(file=here(paste(newFolder2, myFileName2, sep = "/")), height=8, width=10)  #dev.off() is at 400
      
      par(mfrow=c(1,2), mar= c(5.5, 4, 4, 2) + 0.1)
      opar<-par()
      
      # Node counts
      plot(som_model, type = "count", main = "Node Counts", shape = "straight")
      mtext(paste("nodes =", (rows*cols)), side = 1, line = 1)
      mtext(paste("QE = ",round(QE, digits = 3)), side=1, line = 2)
      #mtext(paste("TEbmu =",round(TEbmu, digits = 3)), side=1, line = 3)
      
      # Quality Map
      plot(som_model, type = "quality", main = "Mapping Quality", shape = "straight")
      mtext(paste("Data Set =", myDataSet), side=1, line = 1)
      mtext(paste("Normalization Method =", normMeth), side=1, line = 2)
      mtext(paste("Weighted Inputs =", wghtMeth), side=1, line = 3)
      mtext(paste(myFileName2), side=3, outer=TRUE)
      # A good mapping should show small distances everywhere in the map". (Wehrens & Buydens, 2007)
      # mean distance of objects, mapped to a particular node, to the codebook vector of that node
      
      # check3 <- 'check3'
      # print(check3)
      
      # Training Map
      plot(som_model, type="changes", main = "Training Progress")
      
      # U-Matrix
      plot(som_model, type = "dist.neighbours", main = "U Matrix", shape = "straight", palette.name=grey.colors)
      
      # Codes/ Weight Vectors #default type = "codes"
      plot(som_model, main = "Weight Vectors", shape = "straight", codeRendering = "segments")
      
      # Hierarchical Clustering plot
      #plot(hclust(dist(som_model$codes)))   #OLD CODE
      plot(hclust(code.dists))
      
      # K-Means Clustering plot  ##NEW CODE: had to convert som_model$codes to data frame by adding as.data.frame(som_model$codes)
      wss <- (nrow(as.data.frame(som_model$codes))-1)*sum(apply(as.data.frame(som_model$codes),2,var)) 
      for (k in 2:8) {  #this can be no more than min(grid elements) minus 1 
            wss[k] <- sum(kmeans(as.data.frame(som_model$codes), centers=k)$withinss)
      }
      plot(wss, main= "K-Means Clustering")
      
      # plot the clustering results and add cluster boundaries: 
      plot(som_model, type="mapping", bgcol = barPalette[som_cluster], shape = "straight", main = "Clusters")
      add.cluster.boundaries(som_model, som_cluster) #this is a function within kohonen
      
      # U-Matrix with cluster boundaries
      plot(som_model, type = "dist.neighbours", main = "U Matrix", shape = "straight", palette.name=grey.colors)
      add.cluster.boundaries(som_model, som_cluster)
      
      # check4 <- 'check4'
      # print(check4)
      
      
      # Component Planes (Heat Maps) - Standardized Variable
      # Plot the component planes for each variable using a loop:
      ### NEW CODE: had to convert som_model$codes to a data frame in each call below
      par(mfrow = c(3, 3))
      par(cex = 0.6)
      par(mar = c(3, 3, 0, 0), oma = c(0, 0, 0, 0))
      #par(mgp = c(2, 0.5, 0))
      
      #par(mfrow=c(4,5), mai=c(0.2,0.4,0.2,0))  #mar = c(2,1,1,1), oma=c(0,0,0,0)
      for (c in 1:ncol(as.data.frame(som_model$codes))) {
            plot(som_model, type = "property", cex.lab = 1.4, border = "gray50", shape = "straight", property = as.data.frame(som_model$codes)[,c], main=colnames(as.data.frame(som_model$codes))[c], palette.name = coolBlueHotRed)
            add.cluster.boundaries(som_model, som_cluster)
      }
      
      # # # Component Planes (Heat Maps) - Unstandardized variable
      # ## NOTE: KU will need to add modification to this code from Shayne Lynn blog to address a 
      # # lattice that has empty nodes.
      # # intention is to map the average unscaled value (for a given variable) for 
      # # all input patterns mapped to each node
      # for (i in 1:ncol(som_model$codes)) {
      #       node_mns <- aggregate(as.numeric(myData_num[,i]), by=list(som_model$unit.classif), 
      #                                 FUN=mean, simplify=TRUE)[,2]
      #       #var_unscaled <- #need to fill in zero values for unoccupied nodes
      #       plot(som_model, type = "property", property=var_unscaled, main=colnames(myData_num)[i], palette.name=coolBlueHotRed)
      # }
      # 
      # # OR could try "reverse scaling" the codebook vectors stored in each node,
      # # by multiplying by the stdev of the original variables and
      # # adding the mean of the original variables
      # # perhaps there is a reverse function for scale() or I will need to calculate and store 
      # # stedev and mean in the early lines of the code for use here
      
      # Plot input patterns assigned to each node onto the clustered map
      par(mfrow=c(1,2), mar=c(6,3,3,1), oma=c(0,0,3,0))
      plot(som_model, type = "mapping" , shape = "straight", labels = som_model$unit.classif, main="Node ID Map (syst chk)") # maps node identification
      #plot(som_model, type = "mapping", labels = som_cluster, col=som_cluster+1, main="Mapping Plot")
      #add.cluster.boundaries(som_model, som_cluster)
      
      # check5 <- 'check5'
      # print(check5)      
      
      # Brittany didn't use the following chunck; commenting out for now
      ### Now match node numbers back to original [nrow] input patterns for plotting
      # newlabels <- cbind(som_model$unit.classif, observ, mySRClass)
      # plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], cex=0.75, main="Input Patterns \nMapped to each Node")
      # #plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], col = sedRegPallette[as.integer(mySRClass)], cex=0.75, main="Input Patterns \nMapped to each Node")
      # add.cluster.boundaries(som_model, som_cluster)
      
      ### Now match the cluster assignments back to the original [nrow] input patterns for plotting:
      # such as the line graphs with color coding for clusters that Peter Isles made
      # som_model$unit.classif # this is the node assignments for each data point
      # som_cluster # this is the cluster assignment for each node
      cluster_assign<-som_cluster[som_model$unit.classif] 
      # each data point, could be appended to the original dataset 
      cluster_assign
      
      # Now plot scaled variables by cluster as they differ from the mean
      par(mfrow=c(3,2), mar=c(7,3,3,1), oma=c(0,0,3,0))
      
      #p <- 1 #use this line to break loop for debugging
      for (p in 1:length(unique(cluster_assign))) {
            clusterData<- as.matrix(myData.sc[which(cluster_assign==p),], byrow = TRUE)
            if (nrow(clusterData) == 17) {
                  clusterData <- t(as.matrix(clusterData))
                  clusterMeans <- clusterData  # in case cluster has only one pattern
                  numPatterns <- 1
            } else {
                  clusterMeans<-apply(clusterData, 2, mean)
                  numPatterns <- nrow(clusterData)
            }
            
            dataMeans<-apply(myData.sc, 2, mean)  #compute overall mean for comparison
            barplot(clusterMeans-dataMeans, names.arg=names(clusterData), ylim = c(-.8,.8), las=3, main=paste("Cluster ", p, sep=""), col=barPalette[p])
            # barplot(clusterMeans-dataMeans, names.arg=names(clusterData), las=3, main=paste("Cluster ", p, sep=""), col=barPalette[p])
            mtext(paste("n =", numPatterns), side = 3, line = -0.5)
      }  #end for p loop  #ylim = c(-1.2, 1.2),
      
      #       # plot bar plots of variable values by cluster
      #       # bottom margin = 6.4 to leave room for x label with vertical hatch mark labels
      #       par(mfrow=c(3,2), mar=c(6.5,3,3,1), oma=c(0,0,3,0))
      #       for (pp in 1:length(unique(cluster_assign))) {
      #             clusterData<- as.matrix(myData.sc[which(cluster_assign==pp),], byrow = TRUE)
      #             if (nrow(clusterData) >1) {
      #                   numPatterns <- nrow(clusterData)
      #             } else {
      #                   clusterData <- t(as.matrix(clusterData)) # in case cluster has only one pattern
      #                   numPatterns <- 1
      #             }
      #             boxplot(clusterData ~ colnames(clusterData), xlab = "", ylab = "", 
      #             names=paste(colnames(clusterData)), las=2, cex=2, pch="*", col=barPalette[pp], 
      #             main=paste("Cluster ", pp, sep=""))
      #             mtext(paste("n =", numPatterns), side=3, line=0.1, at=0.0)
      # 
      #       }  #end for pp loop
      #       
      #repeat cluster plot for reference and using same color coding as bar charts
      par(mfrow=c(1,2), mar=c(6,3,3,1), oma=c(0,0,3,0))
      plot(som_model, type="mapping", shape = "straight", bgcol = barPalette[som_cluster], main = "Clusters")
      add.cluster.boundaries(som_model, som_cluster) 
      #mtext(paste("group mean - data mean for vars", normMeth), side=3, outer=TRUE)
      
      # Brittany did not use the next 2 chunks; commenting out for now
      #repeat plot of input patterns mapped to each node for ref next to cluster plot
      # plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], cex=0.75, main="Input Patterns \nMapped to each Node")
      # add.cluster.boundaries(som_model, som_cluster)
      
      #repeat large plot of input patterns mapped to each node for ref next to cluster plot
      # par(mfrow=c(1,1), mar=c(6,3,3,1), oma=c(0,0,3,0))
      # plot(som_model, type = "mapping", shape = "straight", labels = newlabels[ ,2], col = sedRegPallette[as.integer(mySRClass)], cex=1.1, main="Input Patterns \nMapped to each Node \nColored by Mike Class")
      # add.cluster.boundaries(som_model, som_cluster)
      
      
      
      ###############
      #calculate nonparametric F statistic
      
      #first create dataframe of cluster assignments to call from the adonis2() function
      obsNum <- as.character(paste0("obs",observ))
      clustAssignDF <- as.data.frame(cbind(obsNum, cluster_assign))
      colnames(clustAssignDF) <- c('obsNum','Cluster') 
      #then run NP F statistic using adonis function of vegan package
      npFtest <- adonis2(obj.dists ~ Cluster, data=clustAssignDF, by = "terms", permutations=99)
      #extract F ratio and pVal for storage in results table
      npF <- npFtest$F[1]
      pVal_npF <- npFtest$`Pr(>F)`[1]
      
      # check6 <- 'check6'
      # print(check6)
      
      # pp <- 1 #use this line to break loop for debugging
      # #for (pp in 1:length(unique(cluster_assign))) {
      #   finalWghtsData<- as.matrix(myData.sc[which(cluster_assign==pp),], byrow = TRUE)
      #   if (nrow(clusterData) == 18) {
      #     clusterData <- t(as.matrix(clusterData))
      #     clusterMeans <- clusterData  # in case cluster has only one pattern
      #     numPatterns <- 1
      #   } else {
      #     clusterMeans<-apply(clusterData, 2, mean)
      #     numPatterns <- nrow(clusterData)
      #   }
      #   
      #   dataMeans<-apply(myData.sc, 2, mean)  #compute overall mean for comparison
      #   barplot(clusterMeans-dataMeans, ylim = c(-1, 2.5), names.arg=names(clusterData), las=3, main=paste("Cluster ", pp, sep=""), col=barPalette[pp])
      #   mtext(paste("n =", numPatterns), side = 3, line = -0.5)
      # #}  #end for pp loop
      
      
      dev.off()
      
      # ------------------- Track and Store results of each Run -------------------
      Results[i,14]<- mean(som_model$distances)
      Results[i,15] <- round(QE, digits = 6)
      Results[i,16] <- round(npF, digits = 6)
      Results[i,17] <- round(pVal_npF, digits = 6)
      
      clustAssignments <- rbind(clustAssignments, cluster_assign)
        
      
} # end for loop of SOM runs

# Print methods used
noquote(paste0("Topology: ", params[i,4]))
noquote(paste0("Data normalization method used: ", params[i,7]))
noquote(paste0("Weighting method used: ", params[i,8]))
noquote(paste0("No. of iterations: ", params[i,9]))
noquote(paste0("alphaCrs used: ", params[i,10]))
noquote(paste0("alphaFin used: ", params[i,9]))


# FORMAT & EXPORT RESULTS TABLES ----
colnames(Results) <- c('Run','DataSet','Nodes','Topol','rows','cols','normMeth', 
                       'wghtMeth','niter', 'alphaCrs', 'alphaFin', 
                       'Clusters', 'ColRowRat', 'Dist_mn','QE','npF','pVal_npF','blank') 

ResultsDF <- as_tibble(Results) %>% type_convert(col_types = cols(Run = col_double(), cols = col_double()))
write_csv(ResultsDF, here(paste0(newFolder,"/", Sys.Date(), "/", "Results_", myDataSet,".csv")))

clustAssignm <- clustAssignments[-1, ]
colnames(clustAssignm) <- as.character(paste0("obs",observ))
# runID <- as.matrix(params[,1])
# colnames(runID)<- c("Run")
clustAssignm <- cbind(Results, clustAssignm)

clustAssignmDF <- as_tibble(clustAssignm)
write_csv(clustAssignmDF, here(paste0(newFolder,"/", Sys.Date(), "/", "ClustAssign_", myDataSet,".csv")))
```
<br><br>

##### Choose the best SOM run based on non-parametric F-stat and quantization error
We want to maximize npF (ratio of b/w cluster variance) and minimize QE (mean distance b/w each data vector & best-matching unit)  
Here are the top 50% of runs based on npF
```{r choose best lattice config, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.width = 8, out.width = "80%"}
# CHOOSE BEST LATTICE CONFIGURATION ----
  # Examine the non-parametric F-stat (npF) and quantization error (QE) to choose the best lattice configuration
  # You want to maximize npF (maximize b/w cluster variance while minimizing w/i cluster variance) &
  # Minimize QE (avg distance b/w each data vector & best-matching unit [BMU])

  # Set coefficient for second axis on plot below
  # Note: this might not be perfect and may need to adjust
  coeff <- median(ResultsDF$npF)/max(ResultsDF$QE)
  
  ResultsDF %>% 
    # Sort by npF (max to min)
    arrange(desc(npF)) %>% 
    # Only show top 50% of choices
    slice(1:(nrow(.)/2)) %>%
    # Create lattice config ID (rows x cols)
    mutate(latID = paste(paste(rows, "x", cols, sep = ""), "_", Clusters, "cl", "_", Run, sep = "")) %>% 
    # Plot
    ggplot(aes(x = reorder(latID, -npF))) +
    geom_bar(aes(y = npF, fill = "npF"), stat= "identity") +
    geom_line(aes(y = QE * coeff, group = 1, color = "QE")) +
    geom_point(aes(y = QE * coeff, group = 1, color = "QE"), size = 2) +
    scale_y_continuous(
      name = "npF",
      sec.axis = sec_axis(~. / coeff, name = "QE")
    ) +
    scale_fill_manual(values = "gray70") +
    scale_color_manual(values = "blue") +
    xlab("Lattice config. & clusters (rows x cols _ clusters_run)") +
    theme_bw() +
    theme(legend.title = element_blank(),
          legend.spacing = unit(-0.5, "lines"),
          panel.grid = element_blank(),
          axis.text.x = element_text(angle = 90, vjust = 0.5),
          axis.title.x = element_text(margin = margin(t = 5, r = 0, b = 0, l = 0)))  
  
  # Save plot
  # ggsave(here(newFolder, "npFvsQE.pdf"), device = "pdf", height = 4, width = 6, units = "in", dpi = 75)
```
<br><br>

##### Choose the best run from each high performing (top 50%) cluster #
```{r choose best run, include = TRUE, echo = FALSE, message = FALSE}
# Trying to automate the selection process
best_summ_top <- 
  ResultsDF %>% 
  select(Run, rows, cols, Nodes, Clusters, npF, QE) %>% 
  # Sort by npF (max to min)
  arrange(desc(npF)) %>% 
  # Only show top 50% of choices
  slice(1:(nrow(.)/2)) %>%
  # Arrange by QE (min to max)
  arrange(Clusters, QE) %>% 
  # Select the top (lowest QE) for each cluster #
  group_by(Clusters) %>%
  slice(1:1) %>%
  # Sort by npF (max to min)
  arrange(desc(npF)) %>% 
  ungroup()

# Manual selection of SOM runs if above doesn't choose well
best_summ_top_man <- 
  ResultsDF %>% 
  select(Run, rows, cols, Nodes, Clusters, npF, QE) %>% 
  # Choose run numbers here
  filter(Run %in% c(9, 17, 22)) %>% 
  # Sort by npF (max to min)
  arrange(desc(npF)) %>% 
  ungroup()

noquote("These were the top runs for each high-performing cluster #:")
# best_summ_top %>% 
best_summ_top_man %>%   
  mutate_at(vars(c(npF, QE)),
            .funs = ~round(., 3)) %>% 
  gt()

# Select each of the top cluster #s
best_summ_top_3cl <-
  best_summ_top %>% 
  filter(Clusters == 3)

best_summ_top_4cl <-
  best_summ_top %>% 
  filter(Clusters == 4)

best_summ_top_5cl <-
  best_summ_top %>% 
  filter(Clusters == 5)

# best_summ_top1 <-
#   best_summ_top %>%
#   slice(1:1)

# noquote("The script chose:")
# best_summ_top1 %>% 
#   mutate_at(vars(c(npF, QE)),
#             .funs = ~round(., 3)) %>%   
#   gt()

# Choose lattice dimensions you want to examine based on npf/QE plot above
  # n_rows = best_summ_top1$rows
  # n_cols = best_summ_top1$cols
  # n_clust = best_summ_top1$Clusters
  
# Set up dataframes for each of the cluster options
  best_run_3cl = best_summ_top_3cl$Run
  best_run_4cl = best_summ_top_4cl$Run
  best_run_5cl = best_summ_top_5cl$Run
  
# Which Run is best?
  # best_run = best_summ_top1$Run

# CREATE DF WITH EVENT IDs & CLUSTER #'s ----  
  # 3 clusters
  datWithCluster_3cl <- clustAssignmDF %>% 
    # Parse columns into correct type
    type_convert() %>% 
    filter(Run == best_run_3cl) %>% 
    # Pivot to longer format
    pivot_longer(cols = obs1:ncol(.), names_to = "obs", values_to = "cluster") %>% 
    select(cluster) %>% 
    # Bind cluster ID to myData
    bind_cols(myData) %>% 
    # Join to original data to get event IDs
    left_join(mets_sub) %>% 
    # Drop all the independent variables you didn't use in the SOM (copy from myData above); 
    # but KEEP the INFO vars
    select(-one_of(drop.vars)) %>%
    mutate(SOMversion = "3cl") %>% 
    # Arrange columns
    select(SOMversion, site, wyear, date, doy_wyear, cluster, q_mm, q_mm_cum, everything())
  
  # 4 clusters
  datWithCluster_4cl <- clustAssignmDF %>% 
    # Parse columns into correct type
    type_convert() %>% 
    filter(Run == best_run_4cl) %>% 
    # Pivot to longer format
    pivot_longer(cols = obs1:ncol(.), names_to = "obs", values_to = "cluster") %>% 
    select(cluster) %>% 
    # Bind cluster ID to myData
    bind_cols(myData) %>% 
    # Join to original data to get event IDs
    left_join(mets_sub) %>% 
    # Drop all the independent variables you didn't use in the SOM (copy from myData above); 
    # but KEEP the INFO vars
    select(-one_of(drop.vars)) %>%
    mutate(SOMversion = "4cl") %>% 
    # Arrange columns
    select(SOMversion, site, wyear, date, doy_wyear, cluster, q_mm, q_mm_cum, everything())
  
  # 5 clusters
  datWithCluster_5cl <- clustAssignmDF %>% 
    # Parse columns into correct type
    type_convert() %>% 
    filter(Run == best_run_5cl) %>% 
    # Pivot to longer format
    pivot_longer(cols = obs1:ncol(.), names_to = "obs", values_to = "cluster") %>% 
    select(cluster) %>% 
    # Bind cluster ID to myData
    bind_cols(myData) %>% 
    # Join to original data to get event IDs
    left_join(mets_sub) %>% 
    # Drop all the independent variables you didn't use in the SOM (copy from myData above); 
    # but KEEP the INFO vars
    select(-one_of(drop.vars)) %>%
    mutate(SOMversion = "5cl") %>% 
    # Arrange columns
    select(SOMversion, site, wyear, date, doy_wyear, cluster, q_mm, q_mm_cum, everything())
  
  # Combine all into on df
  datWithCluster <-
    bind_rows(datWithCluster_3cl, datWithCluster_4cl, datWithCluster_5cl)
  
  # Write to CSV
  # datWithCluster %>% 
  #   write_csv(here(paste0(newFolder, "/", "SOMresults", "_", myDataSet, "_", Sys.Date(), "_", nclusters, "cl", "_", n_rows, "x", n_cols, ".csv")))
```
<br><br>

> To examine this run in greater detail (e.g., component planes), see the 'X_SOMplots_site_ ... .pdf'  

<br><br>

```{r plot themes and labels, include = FALSE}
# Plotting specifics
  theme1 <- theme_bw() +
            theme(panel.background = element_blank(),
                  panel.grid.major = element_blank(), 
                  panel.grid.minor = element_blank())

  theme2 <- theme_classic() +
            theme(axis.text = element_text(size = 11),
                  axis.title = element_text(size = 12),
                  axis.title.x = element_text(margin=margin(5,0,0,0)),
                  axis.title.y = element_text(margin=margin(0,5,0,0)),
                  legend.title = element_text(size = 9),
                  legend.text = element_text(size = 9))
```

#### 3 clusters - boxplots of independent variables by cluster
```{r boxplots 3cl, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1.4}
# BOXPLOTS OF INDEPENDENT VARIABLES BY CLUSTER ----
  datWithCluster_3cl %>%
    # Remove the drop.info vars from above
    select(-one_of(drop.info)) %>% 
    pivot_longer(cols = precip_mm:pyra_wattm2_dailyTot, names_to = "var", values_to = "value") %>% 
    ggplot(aes(x = cluster, y = value, group = cluster)) +
    facet_wrap(~var, scales = "free_y", ncol = 4) +
    geom_boxplot(fill = "white") +
    geom_point(position=position_jitter(width=0.2), shape=1, size=1, color="gray50", alpha=0.6) +
    ylab("Value") + xlab("Cluster") +
    theme_bw() +
    theme(panel.grid = element_blank(),
          strip.text = element_text(size = 7),
          panel.spacing.x = unit(0.3, "inches"),
          panel.spacing.y = unit(0.45, "inches"))
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1}
# Look at z-scores by cluster
datWithCluster_3cl %>%     
  # Remove the drop.info vars from above
  select(-one_of(drop.info)) %>% 
  # Rename clusters
  mutate(cluster = paste("Cluster", cluster, sep = " ")) %>% 
  # Calculate z-scores for each independent variable
  mutate_at(vars(c(precip_mm:pyra_wattm2_dailyTot)),
          .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  pivot_longer(cols = precip_mm:pyra_wattm2_dailyTot, names_to = "var", values_to = "value") %>% 
  group_by(cluster, var) %>% 
  summarize(median = median(value, na.rm = T),
            q2 = quantile(value, 1/4, na.rm = T),
            q3 = quantile(value, 3/4, na.rm = T)) %>%   
  # Order the vars n var
  # mutate(var = factor(var, levels = c("rain_int_mmPERmin_mean", "rain_int_mmPERmin_max", "rain_event_hrs", "rain_event_total_mm",
  #                                     "q_4d", "time_sinceLastEvent", "API_4d", "VWC_pre_wet_15cm", "VWC_pre_dry_30cm", "VWC_pre_dry_15cm", "SoilTemp_pre_wet_15cm",
  #                                     "solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY"))) %>%
  ggplot(aes(x = var, y = median)) +
    # Must use scales = "free" when using the reordering functions
    facet_wrap(~cluster, ncol = 2) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymin = q2, ymax = q3), width = 0.2, position = position_dodge(0.9)) +
    # scale_x_reordered() +
    ylab("Median z-scores") +
    coord_flip() +
    theme_bw() +
    theme(panel.grid = element_blank(),
          axis.title.y = element_blank()) +
  ggtitle("Characteristics of each cluster - median z-scores")
```
<br><br>

#### 3 clusters - here's how the clusters map on to the cumulative runoff curves
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1, out.width = "90%"}
datWithCluster_3cl %>% 
  mutate(cluster = factor(cluster)) %>% 
  ggplot(aes(x = date)) +
  facet_wrap(~wyear, ncol = 4, scales = "free_x") +
  geom_rect(aes(xmin = date, xmax = date,
                ymin = -Inf, ymax = Inf,
            color = cluster, fill = cluster)) +  
  geom_path(aes(y = q_mm_cum, group = site, linetype = site)) +
  ylab("Runoff (mm)") +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  scale_color_manual(values = c("#56B4E9", "#009E73", "#0072B2")) +
  scale_fill_manual(values = c("#56B4E9", "#009E73", "#0072B2")) +
  # scale_color_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#CC79A7")) +
  # scale_fill_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#CC79A7")) +
  theme1 +
  theme(axis.title.x = element_blank())
```
<br><br>

#### 4 clusters - boxplots of independent variables by cluster
```{r boxplots 4cl, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1.4}
# BOXPLOTS OF INDEPENDENT VARIABLES BY CLUSTER ----
  datWithCluster_4cl %>%
    # Remove the drop.info vars from above
    select(-one_of(drop.info)) %>% 
    pivot_longer(cols = precip_mm:pyra_wattm2_dailyTot, names_to = "var", values_to = "value") %>% 
    ggplot(aes(x = cluster, y = value, group = cluster)) +
    facet_wrap(~var, scales = "free_y", ncol = 4) +
    geom_boxplot(fill = "white") +
    geom_point(position=position_jitter(width=0.2), shape=1, size=1, color="gray50", alpha=0.6) +
    ylab("Value") + xlab("Cluster") +
    theme_bw() +
    theme(panel.grid = element_blank(),
          strip.text = element_text(size = 7),
          panel.spacing.x = unit(0.3, "inches"),
          panel.spacing.y = unit(0.45, "inches"))
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1}
# Look at z-scores by cluster
datWithCluster_4cl %>%     
  # Remove the drop.info vars from above
  select(-one_of(drop.info)) %>% 
  # Rename clusters
  mutate(cluster = paste("Cluster", cluster, sep = " ")) %>% 
  # Calculate z-scores for each independent variable
  mutate_at(vars(c(precip_mm:pyra_wattm2_dailyTot)),
          .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  pivot_longer(cols = precip_mm:pyra_wattm2_dailyTot, names_to = "var", values_to = "value") %>% 
  group_by(cluster, var) %>% 
  summarize(median = median(value, na.rm = T),
            q2 = quantile(value, 1/4, na.rm = T),
            q3 = quantile(value, 3/4, na.rm = T)) %>%   
  # Order the vars n var
  # mutate(var = factor(var, levels = c("rain_int_mmPERmin_mean", "rain_int_mmPERmin_max", "rain_event_hrs", "rain_event_total_mm",
  #                                     "q_4d", "time_sinceLastEvent", "API_4d", "VWC_pre_wet_15cm", "VWC_pre_dry_30cm", "VWC_pre_dry_15cm", "SoilTemp_pre_wet_15cm",
  #                                     "solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY"))) %>%
  ggplot(aes(x = var, y = median)) +
    # Must use scales = "free" when using the reordering functions
    facet_wrap(~cluster, ncol = 2) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymin = q2, ymax = q3), width = 0.2, position = position_dodge(0.9)) +
    # scale_x_reordered() +
    ylab("Median z-scores") +
    coord_flip() +
    theme_bw() +
    theme(panel.grid = element_blank(),
          axis.title.y = element_blank()) +
  ggtitle("Characteristics of each cluster - median z-scores")
```
<br><br>

#### 4 clusters - here's how the clusters map on to the cumulative runoff curves
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1, out.width = "90%"}
datWithCluster_4cl %>% 
  mutate(cluster = factor(cluster)) %>% 
  ggplot(aes(x = date)) +
  facet_wrap(~wyear, ncol = 4, scales = "free_x") +
  geom_rect(aes(xmin = date, xmax = date,
                ymin = -Inf, ymax = Inf,
            color = cluster, fill = cluster)) +  
  geom_path(aes(y = q_mm_cum, group = site, linetype = site)) +
  ylab("Runoff (mm)") +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  scale_color_manual(values = c("#56B4E9", "#009E73", "#E69F00", "#0072B2", "#CC79A7")) +
  scale_fill_manual(values = c("#56B4E9", "#009E73", "#E69F00", "#0072B2", "#CC79A7")) +  
  # scale_color_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#CC79A7")) +
  # scale_fill_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#CC79A7")) +  
  theme1 +
  theme(axis.title.x = element_blank())

# Snow depth curves
# mets %>%
#   filter(doy_wyear >= 145 & doy_wyear <= 245) %>%
#   filter(site == "WB") %>%
#   ggplot(aes(x = date, y = snow_dep_m)) +
#   facet_wrap(~wyear, scales = "free_x", ncol = 4) +
#   geom_path() +
#   scale_x_date(date_breaks = "1 month", date_labels = "%b")
```
<br><br>

#### 4 clusters - here's how the window of analysis maps onto the entire cumulative runoff curves
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1, out.width = "90%"}
mets %>% 
  select(-one_of(drop.vars)) %>%
  full_join(datWithCluster_4cl) %>% 
  arrange(site, date) %>% 
  mutate(cluster = factor(cluster)) %>% 
  filter(!wyear %in% c(1999, 2000)) %>% 
  filter(!is.na(site)) %>% 
  ggplot(aes(x = doy_wyear)) +
  facet_wrap(~wyear, ncol = 4) +
  geom_rect(aes(xmin = doy_wyear, xmax = doy_wyear,
                ymin = -Inf, ymax = Inf,
            color = cluster, fill = cluster)) +
  geom_path(aes(y = q_mm_cum, group = site, linetype = site)) +
  ylab("Runoff (mm)") +
  # scale_x_date(date_breaks = "2 month", date_labels = "%b") +
  scale_color_manual(values = c("#56B4E9", "#009E73", "#E69F00", "#0072B2", "#CC79A7")) +
  scale_fill_manual(values = c("#56B4E9", "#009E73", "#E69F00", "#0072B2", "#CC79A7")) +  
  # scale_color_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#CC79A7")) +
  # scale_fill_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#CC79A7")) + 
  theme1 +
  theme(axis.title.x = element_blank())  
```
<br><br>

#### 4 clusters - exploratory data analysis with SOM results
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
# Summarize # and % of clusters for each analysis window
# Number of observations for each water year (only need it for 1 site)
n_obs <-
  datWithCluster %>% 
  filter(site == "WB") %>% 
  count(SOMversion, wyear) %>% 
  rename(total_obs = n)

# Number of observations in each cluster
# For the entire window of analysis
n_cl_all <-
  datWithCluster %>% 
  filter(site == "WB") %>% 
  mutate(cluster = factor(cluster)) %>% 
  count(SOMversion, wyear, cluster) %>% 
  rename(n_cl_all = n)

clSumm <-
  full_join(n_cl_all, n_obs) %>% 
  # Calculate what proportion of the total obs is each cluster type 
  mutate(n_cl_all_prop = n_cl_all/total_obs) %>% 
  # Join the yield difference calcs
  full_join(yields_comp) %>% 
  select(-c(ends_with("end")))
```

##### Does the distribution of clusters across the spring window of analysis explain spring runoff differences?
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
# Stacked bar graphs
# df for the stacked bar graphs
clSumm3_4cl <-
  clSumm %>% 
  filter(SOMversion == "4cl") %>% 
  mutate(clContr_abs = n_cl_all_prop * WBminusRB_abs_spr,
         clContr_per = n_cl_all_prop * WBminusRB_per_spr)
  
clSumm3_4cl %>% 
  mutate(cluster = factor(cluster, levels = c("4", "3", "2", "1"), exclude = "5")) %>% 
  ggplot(aes(x = reorder(wyear, -WBminusRB_abs_spr), y = clContr_abs, fill = cluster)) +
  geom_bar(position = "stack", stat = "identity") +
  scale_fill_manual(values = c("#0072B2", "#E69F00", "#009E73", "#56B4E9")) +
  # scale_fill_manual(values = c("#56B4E9", "#009E73", "#E69F00", "#0072B2")) +
  xlab("Water year") +
  ylab("Runoff difference in spring (mm; WB - RB)") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90))

clSumm3_4cl %>% 
  mutate(cluster = factor(cluster, levels = c("4", "3", "2", "1"), exclude = "5")) %>% 
  ggplot(aes(x = reorder(wyear, -WBminusRB_abs_spr), y = n_cl_all_prop*100, fill = cluster)) +
  geom_bar(position = "stack", stat = "identity") +
  xlab("Water year") +
  ylab("Runoff difference in spring (%; WB - RB)") +
  scale_fill_manual(values = c("#0072B2", "#E69F00", "#009E73", "#56B4E9")) +
  # scale_fill_manual(values = c("#56B4E9", "#009E73", "#E69F00", "#0072B2")) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90))
```
<br><br>

##### Does the number of warm days with greater than average precip explain the spring runoff difference?
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
# Is there a relationship between the # of the precip+ clusters and the yield diff?
clSumm2_4cl <-
  clSumm %>% 
  filter(SOMversion == "4cl") %>% 
  pivot_wider(names_from = cluster, values_from = c(n_cl_all, n_cl_all_prop)) %>% 
  mutate_all(~ replace_na(., 0)) %>% 
  mutate(n_cl_all_3and4 = n_cl_all_3 + n_cl_all_4,
         n_cl_all_prop_3and4 = n_cl_all_prop_3 + n_cl_all_prop_4)

# Predicting absolute diff.
clSumm2_4cl %>% 
  ggplot(aes(x = n_cl_all_4, y = WBminusRB_abs_spr)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("No. of days as cluster 4") +
  ylab("Runoff difference in spring (mm; WB - RB)") +
  theme_classic()

lm1 <- lm(WBminusRB_abs_spr ~ n_cl_all_4, data = clSumm2_4cl)
r2_lm1 <- round(summary(lm1)$adj.r.squared, 2)
p_lm1 <- round(summary(lm1)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm1))
noquote(paste0("p = ", p_lm1))

# Predicting percent diff.
clSumm2_4cl %>% 
  ggplot(aes(x = n_cl_all_4, y = WBminusRB_per_spr)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("No. of days as cluster 4") +
  ylab("Runoff difference in spring (%; WB - RB)") +
  theme_classic()

lm2 <- lm(WBminusRB_per_spr ~ n_cl_all_4, data = clSumm2_4cl)
r2_lm2 <- round(summary(lm2)$adj.r.squared, 2)
p_lm2 <- round(summary(lm2)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm2))
noquote(paste0("p = ", p_lm2))

clSumm2_4cl %>% 
  ggplot(aes(x = n_cl_all_3and4, y = WBminusRB_abs_spr)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("No. of days as clusters 3 + 4") +
  ylab("Runoff difference in spring (mm; WB - RB)") +  
  theme_classic()

lm3 <- lm(WBminusRB_abs_spr ~ n_cl_all_3and4, data = clSumm2_4cl)
r2_lm3 <- round(summary(lm3)$adj.r.squared, 2)
p_lm3 <- round(summary(lm3)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm3))
noquote(paste0("p = ", p_lm3))
```
<br><br>

##### The runoff diff ~ number of days as cluster 4 is promising, but not perfect; what about total precip during this period?
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
sub_precip <-
  datWithCluster %>% 
  filter(site == "WB") %>% 
  filter(SOMversion == "4cl") %>% 
  group_by(wyear) %>%
  summarize(precip_mm_sub = sum(precip_mm)) %>%
  full_join(yields_comp)

sub_precip %>%
  ggplot(aes(x = precip_mm_sub, y = WBminusRB_abs_spr)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("Total precip. during spring window (mm)") +
  ylab("Runoff difference in spring (mm; WB - RB)") + 
  theme_classic()

lm4 <- lm(WBminusRB_abs_spr ~ precip_mm_sub, data = sub_precip)
r2_lm4 <- round(summary(lm4)$adj.r.squared, 2)
p_lm4 <- round(summary(lm4)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm4))
noquote(paste0("p = ", p_lm4))
```
<br><br>

##### For many years, the divergence occurs after switch from colder to warmer days (from cluster 1 to 2)  
##### Does focusing on the warmer period yield any insight?  
Here I use a running mode to identify when the switch occurs from predominantly cluster 1 to 2 or 4 (cold to warm transition)  
Transition is plotted as the vertical black line  
Note: the running mode (width = 3) worked well for all years but 2017; I shifted that one to first occurrence of cluster 2
<br><br>
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1, out.width = "90%"}
# Let's try identifying where the cluster time series switches from predominantly cold to warm
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

mode_4cl <- 
  datWithCluster %>% 
  filter(SOMversion == "4cl") %>% 
  filter(site == "WB") %>% 
  group_by(wyear) %>% 
  mutate(cluster_mode = rollapply(data = cluster,
                                  width = 3,
                                  FUN = Mode,
                                  align = "center",
                                  fill = NA))

mode_4cl_first <-
  mode_4cl %>%
  # Slice from the first occurrence of 2 as the cluster mode to the end of the time series
  group_by(wyear) %>%
  slice(ifelse(wyear == 2017, min(which(cluster %in% c(2, 4))), min(which(cluster_mode %in% c(2, 4))))) %>%
  select(wyear, date)


datWithCluster_4cl %>%
  mutate(cluster = factor(cluster)) %>%
  ggplot(aes(x = date)) +
  facet_wrap(~wyear, ncol = 4, scales = "free_x") +
  geom_rect(aes(xmin = date, xmax = date,
                ymin = -Inf, ymax = Inf,
            color = cluster, fill = cluster)) +
  geom_vline(data = mode_4cl_first, aes(xintercept = date), size = 0.9) +
  geom_path(aes(y = q_mm_cum, group = site, linetype = site)) +
  ylab("Runoff (mm)") +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  # scale_x_date(date_breaks = "2 month", date_labels = "%b") +
  scale_color_manual(values = c("#56B4E9", "#009E73", "#E69F00", "#0072B2", "#CC79A7")) +
  scale_fill_manual(values = c("#56B4E9", "#009E73", "#E69F00", "#0072B2", "#CC79A7")) +  
  # scale_color_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#CC79A7")) +
  # scale_fill_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#CC79A7")) + 
  theme1 +
  theme(axis.title.x = element_blank())
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
warm_4cl <-
  mode_4cl %>%
  group_by(wyear) %>%
  slice(ifelse(wyear == 2017, min(which(cluster %in% c(2, 4))), min(which(cluster_mode %in% c(2, 4)))):nrow(.)) %>% 
  mutate(period = "warm")

cold_4cl <-
  mode_4cl %>%
  group_by(wyear) %>%
  slice(1:ifelse(wyear == 2017, min(which(cluster %in% c(2, 4))), min(which(cluster_mode %in% c(2, 4))))) %>% 
  mutate(period = "cold")

mode_all <-
  bind_rows(cold_4cl, warm_4cl) %>% 
  filter(site == "WB") %>% 
  count(wyear, period) %>% 
  rename(total_obs_n = n) %>% 
  pivot_wider(names_from = period, values_from = total_obs_n) %>% 
  mutate(prop_warm2cold = warm/cold) %>% 
  full_join(yields_comp)

mode_all %>%
  filter(wyear != 2012) %>% 
  ggplot(aes(x = prop_warm2cold, y = WBminusRB_abs_spr)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("Prop. of days in cold to warm period for whole spring window") +
  ylab("Runoff difference in spring (mm; WB - RB)") +   
  theme_classic() +
  ggtitle("end of spring - excludes 2012")

lm5 <- lm(WBminusRB_abs_spr ~ prop_warm2cold, data = mode_all)
r2_lm5 <- round(summary(lm5)$adj.r.squared, 2)
p_lm5 <- round(summary(lm5)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm5))
noquote(paste0("p = ", p_lm5))

mode_all %>%
  filter(wyear != 2012) %>% 
  ggplot(aes(x = prop_warm2cold, y = WBminusRB_abs_end)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("Prop. of days in cold to warm period for whole spring window") +
  ylab("Runoff difference at year end (mm; WB - RB)") +   
  theme_classic() +
  ggtitle("end of year - excludes 2012")

lm6 <- lm(WBminusRB_abs_end ~ prop_warm2cold, data = mode_all)
r2_lm6 <- round(summary(lm6)$adj.r.squared, 2)
p_lm6 <- round(summary(lm6)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm6))
noquote(paste0("p = ", p_lm6))

# Number of days in warm period
n_obs_warm <-
  warm_4cl %>%
  filter(site == "WB") %>%
  count(wyear) %>%
  rename(total_obs = n)

# Number of observations in each cluster
# For the warm period
n_cl_warm <-
  warm_4cl %>%
  filter(site == "WB") %>%
  mutate(cluster = factor(cluster)) %>%
  count(wyear, cluster) %>%
  rename(n_cl_warm = n)

clSumm_warm_4cl <-
  full_join(n_cl_warm, n_obs_warm) %>%
  # Calculate what proportion of the total obs is each cluster type
  mutate(n_cl_warm_prop = n_cl_warm/total_obs) %>%
  # Join the yield difference calcs
  full_join(yields_comp)

clSumm2_warm_4cl <-
  clSumm_warm_4cl %>%
  pivot_wider(names_from = cluster, values_from = c(n_cl_warm, n_cl_warm_prop)) %>%
  mutate_all(~ replace_na(., 0)) %>%
  mutate(prop_cold2warm = n_cl_warm_1/(n_cl_warm_2 + n_cl_warm_4))

clSumm2_warm_4cl %>%
  ggplot(aes(x = n_cl_warm_4, y = WBminusRB_abs_spr)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("No. of days as cluster 4") +
  ylab("Runoff difference in spring (mm; WB - RB)") +  
  theme_classic() +
  ggtitle("end of spring")

lm7 <- lm(WBminusRB_abs_spr ~ n_cl_warm_4, data = clSumm2_warm_4cl)
r2_lm7 <- round(summary(lm7)$adj.r.squared, 2)
p_lm7 <- round(summary(lm7)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm7))
noquote(paste0("p = ", p_lm7))

clSumm2_warm_4cl %>%
  ggplot(aes(x = n_cl_warm_4, y = WBminusRB_abs_end)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("No. of days as cluster 4") +
  ylab("Runoff difference at year end (mm; WB - RB)") +  
  theme_classic() +
  ggtitle("end of year")

lm8 <- lm(WBminusRB_abs_end ~ n_cl_warm_4, data = clSumm2_warm_4cl)
r2_lm8 <- round(summary(lm8)$adj.r.squared, 2)
p_lm8 <- round(summary(lm8)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm8))
noquote(paste0("p = ", p_lm8))

warm_precip <-
  warm_4cl %>%
  group_by(wyear) %>%
  summarize(precip_mm_warm = sum(precip_mm)) %>%
  full_join(yields_comp) %>%
  full_join(clSumm2_warm_4cl)

warm_precip %>%
  ggplot(aes(x = precip_mm_warm, y = WBminusRB_abs_spr)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("Total precip. during warmer period (mm)") +
  ylab("Runoff difference in spring (mm; WB - RB)") + 
  theme_classic() +
  ggtitle("end of spring")

lm9 <- lm(WBminusRB_abs_spr ~ precip_mm_warm, data = warm_precip)
r2_lm9 <- round(summary(lm9)$adj.r.squared, 2)
p_lm9 <- round(summary(lm9)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm9))
noquote(paste0("p = ", p_lm9))

warm_precip %>%
  ggplot(aes(x = precip_mm_warm, y = WBminusRB_abs_end)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("Total precip. during warmer period (mm)") +
  ylab("Runoff difference at year end (mm; WB - RB)") + 
  theme_classic() +
  ggtitle("end of year")

lm10 <- lm(WBminusRB_abs_end ~ precip_mm_warm, data = warm_precip)
r2_lm10 <- round(summary(lm10)$adj.r.squared, 2)
p_lm10 <- round(summary(lm10)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm10))
noquote(paste0("p = ", p_lm10))

lastcold <-
  datWithCluster %>%
  filter(SOMversion == "4cl") %>%
  filter(site == "WB") %>%
  group_by(wyear) %>%
  # Slice after the last occurrence of 1 as the cluster mode to the end of the time series
  group_by(wyear) %>%
  slice(max(which(cluster == 1)):nrow(.))

lastcold_precip <-
  lastcold %>%
  group_by(wyear) %>%
  summarize(precip_mm_lastcold = sum(precip_mm)) %>%
  full_join(yields_comp)

lastcold_precip %>%
  ggplot(aes(x = precip_mm_lastcold, y = WBminusRB_abs_spr)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("Total precip. after last cold day (cluster 2) (mm)") +
  ylab("Runoff difference in spring (mm; WB - RB)") +   
  theme_classic() +
  ggtitle("end of spring")

lm11 <- lm(WBminusRB_abs_spr ~ precip_mm_lastcold, data = lastcold_precip)
r2_lm11 <- round(summary(lm11)$adj.r.squared, 2)
p_lm11 <- round(summary(lm11)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm11))
noquote(paste0("p = ", p_lm11))

lastcold_precip %>%
  filter(wyear != 2012) %>% 
  ggplot(aes(x = precip_mm_lastcold, y = WBminusRB_abs_spr)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("Total precip. after last cold day (cluster 2) (mm)") +
  ylab("Runoff difference in spring (mm; WB - RB)") +   
  theme_classic() +
  ggtitle("end of spring - same as above, but excludes 2012")

lm12 <- lm(WBminusRB_abs_spr ~ precip_mm_lastcold, data = lastcold_precip %>% filter(wyear != 2012))
r2_lm12 <- round(summary(lm12)$adj.r.squared, 2)
p_lm12 <- round(summary(lm12)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm12))
noquote(paste0("p = ", p_lm12))

lastcold_precip %>%
  ggplot(aes(x = precip_mm_lastcold, y = WBminusRB_abs_end)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(shape = 1) +
  geom_text(aes(label = wyear), hjust = -0.25, vjust = 0, size = 2) +
  xlab("Total precip. after last cold day (cluster 2) (mm)") +
  ylab("Runoff difference at year end (mm; WB - RB)") +   
  theme_classic() +
  ggtitle("end of year")

lm13 <- lm(WBminusRB_abs_end ~ precip_mm_lastcold, data = lastcold_precip)
r2_lm13 <- round(summary(lm13)$adj.r.squared, 2)
p_lm13 <- round(summary(lm13)$coefficients[2,4], 3)
noquote(paste0("R2 = ", r2_lm13))
noquote(paste0("p = ", p_lm13))

# Is there a relationship b/w date of divergence of q_mm_cum (> some threshold) & yield diff?
```
<br><br>

#### 5 clusters - examine boxplots of independent variables by cluster
```{r boxplots 5cl, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1.4}
# BOXPLOTS OF INDEPENDENT VARIABLES BY CLUSTER ----
  datWithCluster_5cl %>%
    # Remove the drop.info vars from above
    select(-one_of(drop.info)) %>% 
    pivot_longer(cols = precip_mm:pyra_wattm2_dailyTot, names_to = "var", values_to = "value") %>% 
    ggplot(aes(x = cluster, y = value, group = cluster)) +
    facet_wrap(~var, scales = "free_y", ncol = 4) +
    geom_boxplot(fill = "white") +
    geom_point(position=position_jitter(width=0.2), shape=1, size=1, color="gray50", alpha=0.6) +
    ylab("Value") + xlab("Cluster") +
    theme_bw() +
    theme(panel.grid = element_blank(),
          strip.text = element_text(size = 7),
          panel.spacing.x = unit(0.3, "inches"),
          panel.spacing.y = unit(0.45, "inches"))
```
<br><br>

```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1}
# Look at z-scores by cluster
datWithCluster_5cl %>%     
  # Remove the drop.info vars from above
  select(-one_of(drop.info)) %>% 
  # Rename clusters
  mutate(cluster = paste("Cluster", cluster, sep = " ")) %>% 
  # Calculate z-scores for each independent variable
  mutate_at(vars(c(precip_mm:pyra_wattm2_dailyTot)),
          .funs = list(~ (. - mean(., na.rm = T)) / sd(., na.rm = T))) %>%
  pivot_longer(cols = precip_mm:pyra_wattm2_dailyTot, names_to = "var", values_to = "value") %>% 
  group_by(cluster, var) %>% 
  summarize(median = median(value, na.rm = T),
            q2 = quantile(value, 1/4, na.rm = T),
            q3 = quantile(value, 3/4, na.rm = T)) %>%   
  # Order the vars n var
  # mutate(var = factor(var, levels = c("rain_int_mmPERmin_mean", "rain_int_mmPERmin_max", "rain_event_hrs", "rain_event_total_mm",
  #                                     "q_4d", "time_sinceLastEvent", "API_4d", "VWC_pre_wet_15cm", "VWC_pre_dry_30cm", "VWC_pre_dry_15cm", "SoilTemp_pre_wet_15cm",
  #                                     "solarRad_4d", "PET_mmHR", "diff_airT_soilT", "DOY"))) %>%
  ggplot(aes(x = var, y = median)) +
    # Must use scales = "free" when using the reordering functions
    facet_wrap(~cluster, ncol = 2) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymin = q2, ymax = q3), width = 0.2, position = position_dodge(0.9)) +
    # scale_x_reordered() +
    ylab("Median z-scores") +
    coord_flip() +
    theme_bw() +
    theme(panel.grid = element_blank(),
          axis.title.y = element_blank()) +
  ggtitle("Characteristics of each cluster - median z-scores")
```
<br><br>

#### 5 clusters - examine how the clusters map on to the cumulative runoff curves
```{r, include = TRUE, echo = FALSE, message = FALSE, warning = FALSE, fig.asp = 1, out.width = "90%"}
datWithCluster_5cl %>% 
  mutate(cluster = factor(cluster)) %>% 
  ggplot(aes(x = date)) +
  facet_wrap(~wyear, ncol = 4, scales = "free_x") +
  geom_rect(aes(xmin = date, xmax = date,
                ymin = -Inf, ymax = Inf,
            color = cluster, fill = cluster)) +  
  geom_path(aes(y = q_mm_cum, group = site, linetype = site)) +
  ylab("Runoff (mm)") +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  scale_color_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#CC79A7")) +
  scale_fill_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#0072B2", "#CC79A7")) +  
  theme1 +
  theme(axis.title.x = element_blank())
```
<br><br>
